{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRAMMAR = \"\"\"\n",
    "    S -> NNP VP\n",
    "    VP -> V PP\n",
    "    PP -> P NP\n",
    "    NP -> DT N\n",
    "    NNP -> 'Gwen' | 'George'\n",
    "    V -> 'looks' | 'burns'\n",
    "    P -> 'in' | 'for'\n",
    "    DT -> 'the'\n",
    "    N -> 'castle' | 'ocean'\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NNP VP\n",
      "    VP -> V PP\n",
      "    PP -> P NP\n",
      "    NP -> DT N\n",
      "    NNP -> 'Gwen'\n",
      "    NNP -> 'George'\n",
      "    V -> 'looks'\n",
      "    V -> 'burns'\n",
      "    P -> 'in'\n",
      "    P -> 'for'\n",
      "    DT -> 'the'\n",
      "    N -> 'castle'\n",
      "    N -> 'ocean'\n",
      "S\n",
      "[S -> NNP VP, VP -> V PP, PP -> P NP, NP -> DT N, NNP -> 'Gwen', NNP -> 'George', V -> 'looks', V -> 'burns', P -> 'in', P -> 'for', DT -> 'the', N -> 'castle', N -> 'ocean']\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG\n",
    "cfg = CFG.fromstring(GRAMMAR)\n",
    "\n",
    "print(cfg)\n",
    "print(cfg.start())\n",
    "print(cfg.productions())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from nltk.chunk.regexp import RegexpParser\n",
    "\n",
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "chunker = RegexpParser(GRAMMAR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-d945ca774714>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mGOODTAGS\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfrozenset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'JJ'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'JJR'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'JJS'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'NN'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'NNP'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'NNS'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'NNPS'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mKeyphraseExtractor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mBaseEstimator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTransformerMixin\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \"\"\"\n\u001B[1;32m      6\u001B[0m     \u001B[0mWraps\u001B[0m \u001B[0ma\u001B[0m \u001B[0mPickledCorpusReader\u001B[0m \u001B[0mconsisting\u001B[0m \u001B[0mof\u001B[0m \u001B[0mpos\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mtagged\u001B[0m \u001B[0mdocuments\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "GOODTAGS = frozenset(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])\n",
    "\n",
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wraps a PickledCorpusReader consisting of pos-tagged documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, grammar=GRAMMAR):\n",
    "        self.grammar = GRAMMAR\n",
    "        self.chunker = RegexpParser(self.grammar)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from unicodedata import category as unicat\n",
    "\n",
    "def normalize(self, sent):\n",
    "    \"\"\"\n",
    "    Removes punctuation from a tokenized/tagged sentence and\n",
    "    lowercases words.\n",
    "    \"\"\"\n",
    "    is_punct = lambda word: all(unicat(c).startswith('P') for c in word)\n",
    "    sent = filter(lambda t: not is_punct(t[0]), sent)\n",
    "    sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
    "    return list(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "def extract_keyphrases(self, document):\n",
    "    \"\"\"\n",
    "    For a document, parse sentences using our chunker created by\n",
    "    our grammar, converting the parse tree into a tagged sequence.\n",
    "    Yields extracted phrases.\n",
    "    \"\"\"\n",
    "    for sents in document:\n",
    "        for sent in sents:\n",
    "            sent = self.normalize(sent)\n",
    "            if not sent: continue\n",
    "            chunks = tree2conlltags(self.chunker.parse(sent))\n",
    "            phrases = [\n",
    "                \" \".join(word for word, pos, chunk in group).lower()\n",
    "                for key, group in groupby(\n",
    "                    chunks, lambda term: term[-1] != 'O'\n",
    "                ) if key\n",
    "            ]\n",
    "            for phrase in phrases:\n",
    "                yield phrase"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def fit(self, documents, y=None):\n",
    "    return self\n",
    "\n",
    "def transform(self, documents):\n",
    "    for document in documents:\n",
    "        yield self.extract_keyphrases(document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-c168369acf69>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mGOODLABELS\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfrozenset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'PERSON'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'ORGANIZATION'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'FACILITY'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'GPE'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'GSP'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mEntityExtractor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mBaseEstimator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTransformerMixin\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mGOODLABELS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "GOODLABELS = frozenset(['PERSON', 'ORGANIZATION', 'FACILITY', 'GPE', 'GSP'])\n",
    "\n",
    "class EntityExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, labels=GOODLABELS, **kwargs):\n",
    "        self.labels = labels\n",
    "\n",
    "    def get_entities(self, document):\n",
    "        entities = []\n",
    "        for paragraph in document:\n",
    "            for sentence in paragraph:\n",
    "                trees = ne_chunk(sentence)\n",
    "                for tree in trees:\n",
    "                    if hasattr(tree, 'label'):\n",
    "                        if tree.label() in self.labels:\n",
    "                            entities.append(\n",
    "                                ' '.join([child[0].lower() for child in tree])\n",
    "                                )\n",
    "        return entities\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.get_entities(document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def ngrams(words, n=2):\n",
    "    for idx in range(len(words)-n+1):\n",
    "        yield tuple(words[idx:idx+n])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'reporters', 'listened')\n",
      "('reporters', 'listened', 'closely')\n",
      "('listened', 'closely', 'as')\n",
      "('closely', 'as', 'the')\n",
      "('as', 'the', 'President')\n",
      "('the', 'President', 'of')\n",
      "('President', 'of', 'the')\n",
      "('of', 'the', 'United')\n",
      "('the', 'United', 'States')\n",
      "('United', 'States', 'addressed')\n",
      "('States', 'addressed', 'the')\n",
      "('addressed', 'the', 'room')\n",
      "('the', 'room', '.')\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    \"The\", \"reporters\", \"listened\", \"closely\", \"as\", \"the\", \"President\",\n",
    "    \"of\", \"the\", \"United\", \"States\", \"addressed\", \"the\", \"room\", \".\",\n",
    "]\n",
    "\n",
    "for ngram in ngrams(words, n=3):\n",
    "    print(ngram)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CategorizedCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-85dce74ca3b0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mHTMLCorpusReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCategorizedCorpusReader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCorpusReader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mngrams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfileids\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcategories\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfileids\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfileids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcategories\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcategories\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mngram\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mngrams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'CategorizedCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "\n",
    "    def ngrams(self, n=2, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids=fileids, categories=categories):\n",
    "            for ngram in nltk.ngrams(sent, n):\n",
    "                yield ngram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from functools import partial\n",
    "\n",
    "LPAD_SYMBOL = \"<s>\"\n",
    "RPAD_SYMBOL = \"</s>\"\n",
    "\n",
    "nltk_ngrams = partial(\n",
    "    nltk.ngrams,\n",
    "    pad_right=True, right_pad_symbol=RPAD_SYMBOL,\n",
    "    left_pad=True, left_pad_symbol=LPAD_SYMBOL\n",
    ")\n",
    "\n",
    "def ngrams(self, n=2, fileids=None, categories=None):\n",
    "    for sent in self.sents(fileids=fileids, categories=categories):\n",
    "        for ngram in nltk.ngrams(sent, n):\n",
    "            yield ngram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from nltk.collocations import QuadgramCollocationFinder\n",
    "from nltk.metrics.association import QuadgramAssocMeasures\n",
    "\n",
    "\n",
    "def rank_quadgrams(corpus, metric, path=None):\n",
    "    \"\"\"\n",
    "    Find and rank quadgrams from the supplied corpus using the given\n",
    "    association metric. Write the quadgrams out to the given path if\n",
    "    supplied otherwise return the list in memory.\n",
    "    \"\"\"\n",
    "    # Create a collocation ranking utility from corpus words.\n",
    "    ngrams = QuadgramCollocationFinder.from_words(corpus.words())\n",
    "\n",
    "    # Rank collocations by an association metric\n",
    "    scored = ngrams.score_ngrams(metric)\n",
    "\n",
    "    if path:\n",
    "        # Write to disk as tab-delimited file\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"Collocation\\tScore ({})\".format(metric.__name__))\n",
    "            for ngram, score in scored:\n",
    "                f.write(\"{}\\t{}\\n\".format(repr(ngram), score))\n",
    "    else:\n",
    "        return scored"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-7b11a2cebb7a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m rank_quadgrams(\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mcorpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mQuadgramAssocMeasures\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlikelihood_ratio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'quadgrams.txt'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m )\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "rank_quadgrams(\n",
    "    corpus, QuadgramAssocMeasures.likelihood_ratio, 'quadgrams.txt'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SignificantCollocations(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ngram_class=QuadgramCollocationFinder,\n",
    "                 metric=QuadgramAssocMeasures.pmi):\n",
    "        self.ngram_class = ngram_class\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, docs, target):\n",
    "        ngrams = self.ngram_class.from_documents(docs)\n",
    "        self.scored_ = dict(ngrams.score_ngrams(self.metric))\n",
    "\n",
    "    def transform(self, docs):\n",
    "        for doc in docs:\n",
    "            ngrams = self.ngram_class.from_words(docs)\n",
    "            yield {\n",
    "                ngram: self.scored_.get(ngram, 0.0)\n",
    "                for ngram in ngrams.nbest(QuadgramAssocMeasures.raw_freq, 50)\n",
    "            }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "<>:8: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "<ipython-input-18-548b859bf53f>:8: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "  ('union', FeatureUnion(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-548b859bf53f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m model = Pipeline([\n\u001B[0;32m----> 8\u001B[0;31m     ('union', FeatureUnion(\n\u001B[0m\u001B[1;32m      9\u001B[0m         transformer_list=[\n\u001B[1;32m     10\u001B[0m             ('ngrams', Pipeline([\n",
      "\u001B[0;31mTypeError\u001B[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('ngrams', Pipeline([\n",
    "                ('sigcol', SignificantCollocations()),\n",
    "                ('dsigcol', DictVectorizer()),\n",
    "            ])),\n",
    "\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "        ]\n",
    "    ))\n",
    "\n",
    "    ('clf', SGDClassifier()),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Padding Symbols\n",
    "UNKNOWN = \"<UNK>\"\n",
    "LPAD = \"<s>\"\n",
    "RPAD = \"</s>\"\n",
    "\n",
    "\n",
    "class NgramCounter(object):\n",
    "    \"\"\"\n",
    "    The NgramCounter class counts ngrams given a vocabulary and ngram size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, vocabulary, unknown=UNKNOWN):\n",
    "        \"\"\"\n",
    "        n is the size of the ngram\n",
    "        \"\"\"\n",
    "        if n < 1:\n",
    "            raise ValueError(\"ngram size must be greater than or equal to 1\")\n",
    "\n",
    "        self.n = n\n",
    "        self.unknown = unknown\n",
    "        self.padding = {\n",
    "            \"pad_left\": True,\n",
    "            \"pad_right\": True,\n",
    "            \"left_pad_symbol\": LPAD,\n",
    "            \"right_pad_symbol\": RPAD,\n",
    "        }\n",
    "        self.vocabulary = vocabulary\n",
    "        self.allgrams = defaultdict(ConditionalFreqDist)\n",
    "        self.ngrams = FreqDist()\n",
    "        self.unigrams = FreqDist()\n",
    "\n",
    "    def train_counts(self, training_text):\n",
    "        for sent in training_text:\n",
    "            checked_sent = (self.check_against_vocab(word) for word in sent)\n",
    "            sent_start = True\n",
    "            for ngram in self.to_ngrams(checked_sent):\n",
    "                self.ngrams[ngram] += 1\n",
    "                context, word = tuple(ngram[:-1]), ngram[-1]\n",
    "                if sent_start:\n",
    "                    for context_word in context:\n",
    "                        self.unigrams[context_word] += 1\n",
    "                    sent_start = False\n",
    "\n",
    "                for window, ngram_order in enumerate(range(self.n, 1, -1)):\n",
    "                    context = context[window:]\n",
    "                    self.allgrams[ngram_order][context][word] += 1\n",
    "                self.unigrams[word] += 1\n",
    "\n",
    "    def check_against_vocab(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            return word\n",
    "        return self.unknown\n",
    "\n",
    "    def to_ngrams(self, sequence):\n",
    "        \"\"\"\n",
    "        Wrapper for NLTK ngrams method\n",
    "        \"\"\"\n",
    "        return ngrams(sequence, self.n, **self.padding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-e53431444375>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'__main__'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPickledCorpusReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/sample'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m     \u001B[0mtokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mvocab\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCounter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "def count_ngrams(n, vocabulary, texts):\n",
    "    counter = NgramCounter(n, vocabulary)\n",
    "    counter.train_counts(texts)\n",
    "    return counter\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = PickledCorpusReader('../data/sample')\n",
    "    tokens = [''.join(word[0]) for word in corpus.words()]\n",
    "    vocab = Counter(tokens)\n",
    "    sents = list([word[0] for word in sent] for sent in corpus.sents())\n",
    "    trigram_counts = count_ngrams(3, vocab, sents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trigram_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-1238b54bffd4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrigram_counts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munigrams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trigram_counts' is not defined"
     ]
    }
   ],
   "source": [
    "print(trigram_counts.unigrams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trigram_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-9fcc01ac19b8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrigram_counts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mngrams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trigram_counts' is not defined"
     ]
    }
   ],
   "source": [
    "print(trigram_counts.ngrams[3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trigram_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-24-03b5dc304d9f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrigram_counts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mngrams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconditions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trigram_counts' is not defined"
     ]
    }
   ],
   "source": [
    "print(sorted(trigram_counts.ngrams[3].conditions()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trigram_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-9b50e26acb9b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrigram_counts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mngrams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'the'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'President'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trigram_counts' is not defined"
     ]
    }
   ],
   "source": [
    "print(list(trigram_counts.ngrams[3][('the', 'President')]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class BaseNgramModel(object):\n",
    "    \"\"\"\n",
    "    The BaseNgramModel creates an n-gram language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ngram_counter):\n",
    "        \"\"\"\n",
    "        BaseNgramModel is initialized with an NgramCounter.\n",
    "        \"\"\"\n",
    "        self.n = ngram_counter.n\n",
    "        self.ngram_counter = ngram_counter\n",
    "        self.ngrams = ngram_counter.ngrams[ngram_counter.n]\n",
    "        self._check_against_vocab = self.ngram_counter.check_against_vocab\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        For a given string representation of a word, and a string word context,\n",
    "        returns the maximum likelihood score that the word will follow the\n",
    "        context.\n",
    "\n",
    "        fdist[context].freq(word) == fdist[(context, word)] / fdist[context]\n",
    "        \"\"\"\n",
    "        context = self.check_context(context)\n",
    "\n",
    "        return self.ngrams[context].freq(word)\n",
    "\n",
    "    def check_context(self, context):\n",
    "        \"\"\"\n",
    "        Ensures that the context is not longer than or equal to the model's\n",
    "        highest n-gram order.\n",
    "\n",
    "        Returns the context as a tuple.\n",
    "        \"\"\"\n",
    "        if len(context) >= self.n:\n",
    "            raise ValueError(\"Context too long for this n-gram\")\n",
    "\n",
    "        return tuple(context)\n",
    "\n",
    "    def logscore(self, word, context):\n",
    "        \"\"\"\n",
    "        For a given string representation of a word, and a word context,\n",
    "        computes the log probability of the word in the context.\n",
    "        \"\"\"\n",
    "        score = self.score(word, context)\n",
    "        if score <= 0.0:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "        return log(score, 2)\n",
    "\n",
    "    def entropy(self, text):\n",
    "        \"\"\"\n",
    "        Calculate the approximate cross-entropy of the n-gram model for a\n",
    "        given text represented as a list of comma-separated strings.\n",
    "        This is the average log probability of each word in the text.\n",
    "        \"\"\"\n",
    "        normed_text = (self._check_against_vocab(word) for word in text)\n",
    "        entropy = 0.0\n",
    "        processed_ngrams = 0\n",
    "        for ngram in self.ngram_counter.to_ngrams(normed_text):\n",
    "            context, word = tuple(ngram[:-1]), ngram[-1]\n",
    "            entropy += self.logscore(word, context)\n",
    "            processed_ngrams += 1\n",
    "        return - (entropy / processed_ngrams)\n",
    "\n",
    "    def perplexity(self, text):\n",
    "        \"\"\"\n",
    "        Given list of comma-separated strings, calculates the perplexity\n",
    "        of the text.\n",
    "        \"\"\"\n",
    "        return pow(2.0, self.entropy(text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-3dffaa7f9519>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrigram_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBaseNgramModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcount_ngrams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msents\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mfivegram_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBaseNgramModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcount_ngrams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msents\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrigram_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperplexity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msents\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfivegram_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperplexity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msents\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "trigram_model = BaseNgramModel(count_ngrams(3, vocab, sents))\n",
    "fivegram_model = BaseNgramModel(count_ngrams(5, vocab, sents))\n",
    "\n",
    "print(trigram_model.perplexity(sents[0]))\n",
    "print(fivegram_model.perplexity(sents[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class AddKNgramModel(BaseNgramModel):\n",
    "    \"\"\"\n",
    "    Provides add-k smoothed scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, k, *args):\n",
    "        \"\"\"\n",
    "        Expects an input value, k, a number by which\n",
    "        to increment word counts during scoring.\n",
    "        \"\"\"\n",
    "        super(AddKNgramModel, self).__init__(*args)\n",
    "\n",
    "        self.k = k\n",
    "        self.k_norm = len(self.ngram_counter.vocabulary) * k\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        With Add-k-smoothing, the score is normalized with\n",
    "        a k value.\n",
    "        \"\"\"\n",
    "        context = self.check_context(context)\n",
    "        context_freqdist = self.ngrams[context]\n",
    "        word_count = context_freqdist[word]\n",
    "        context_count = context_freqdist.N()\n",
    "        return (word_count + self.k) / \\\n",
    "               (context_count + self.k_norm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class LaplaceNgramModel(AddKNgramModel):\n",
    "    \"\"\"\n",
    "    Implements Laplace (add one) smoothing.\n",
    "    Laplace smoothing is the base case of add-k smoothing,\n",
    "    with k set to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(LaplaceNgramModel, self).__init__(1, *args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class KneserNeyModel(BaseNgramModel):\n",
    "    \"\"\"\n",
    "    Implements Kneser-Ney smoothing\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(KneserNeyModel, self).__init__(*args)\n",
    "        self.model = nltk.KneserNeyProbDist(self.ngrams)\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        Use KneserNeyProbDist from NLTK to get score\n",
    "        \"\"\"\n",
    "        trigram = tuple((context[0], context[1], word))\n",
    "        return self.model.prob(trigram)\n",
    "\n",
    "    def samples(self):\n",
    "        return self.model.samples()\n",
    "\n",
    "    def prob(self, sample):\n",
    "        return self.model.prob(sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-31-54dd71181242>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPickledCorpusReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/sample'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mvocab\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCounter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0msents\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msent\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = PickledCorpusReader('../data/sample')\n",
    "tokens = [''.join(word) for word in corpus.words()]\n",
    "vocab = Counter(tokens)\n",
    "sents = list([word[0] for word in sent] for sent in corpus.sents())\n",
    "\n",
    "counter = count_ngrams(3, vocab, sents)\n",
    "knm = KneserNeyModel(counter)\n",
    "\n",
    "def complete(input_text):\n",
    "    tokenized = nltk.word_tokenize(input_text)\n",
    "    if len(tokenized) < 2:\n",
    "        response = \"Say more.\"\n",
    "    else:\n",
    "        completions = {}\n",
    "        for sample in knm.samples():\n",
    "            if (sample[0], sample[1]) == (tokenized[-2], tokenized[-1]):\n",
    "                completions[sample[2]] = knm.prob(sample)\n",
    "        if len(completions) == 0:\n",
    "            response = \"Can we talk about something else?\"\n",
    "        else:\n",
    "            best = max(\n",
    "                completions.keys(), key=(lambda key: completions[key])\n",
    "            )\n",
    "            tokenized += [best]\n",
    "            response = \" \".join(tokenized)\n",
    "\n",
    "    return response\n",
    "\n",
    "print(complete(\"The President of the United\"))\n",
    "print(complete(\"This election year will\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}