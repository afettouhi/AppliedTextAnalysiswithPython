{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from readability.readability import Unparseable\n",
    "from readability.readability import Document as Paper\n",
    "\n",
    "def html(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    Returns the HTML content of each document, cleaning it using\n",
    "    the readability-lxml library.\n",
    "    \"\"\"\n",
    "    for doc in self.docs(fileids, categories):\n",
    "        try:\n",
    "            yield Paper(doc).summary()\n",
    "        except Unparseable as e:\n",
    "            print(\"Could not parse HTML: {}\".format(e))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(\"readability.readability\")\n",
    "log.setLevel('WARNING')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "# Tags to extract as paragraphs from the HTML text\n",
    "tags = [\n",
    "    'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li'\n",
    "]\n",
    "\n",
    "def paras(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    Uses BeautifulSoup to parse the paragraphs from the HTML.\n",
    "    \"\"\"\n",
    "    for html in self.html(fileids, categories):\n",
    "        soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "        for element in soup.find_all(tags):\n",
    "            yield element.text\n",
    "        soup.decompose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "def sents(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    Uses the built in sentence tokenizer to extract sentences from the\n",
    "    paragraphs. Note that this method uses BeautifulSoup to parse HTML.\n",
    "    \"\"\"\n",
    "\n",
    "    for paragraph in self.paras(fileids, categories):\n",
    "        for sentence in sent_tokenize(paragraph):\n",
    "            yield sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "def words(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    Uses the built-in word tokenizer to extract tokens from sentences.\n",
    "    Note that this method uses BeautifulSoup to parse HTML content.\n",
    "    \"\"\"\n",
    "    for sentence in self.sents(fileids, categories):\n",
    "        for token in wordpunct_tokenize(sentence):\n",
    "            yield token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "\n",
    "def tokenize(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    Segments, tokenizes, and tags a document in the corpus.\n",
    "    \"\"\"\n",
    "    for paragraph in self.paras(fileids=fileids):\n",
    "        yield [\n",
    "            pos_tag(wordpunct_tokenize(sent))\n",
    "            for sent in sent_tokenize(paragraph)\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "\n",
    "def describe(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    Performs a single pass of the corpus and\n",
    "    returns a dictionary with a variety of metrics\n",
    "    concerning the state of the corpus.\n",
    "    \"\"\"\n",
    "    started = time.time()\n",
    "\n",
    "    # Structures to perform counting.\n",
    "    counts  = nltk.FreqDist()\n",
    "    tokens  = nltk.FreqDist()\n",
    "\n",
    "    # Perform single pass over paragraphs, tokenize and count\n",
    "    for para in self.paras(fileids, categories):\n",
    "        counts['paras'] += 1\n",
    "\n",
    "        for sent in para:\n",
    "            counts['sents'] += 1\n",
    "\n",
    "            for word, tag in sent:\n",
    "                counts['words'] += 1\n",
    "                tokens[word] += 1\n",
    "\n",
    "    # Compute the number of files and categories in the corpus\n",
    "    n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
    "    n_topics  = len(self.categories(self.resolve(fileids, categories)))\n",
    "\n",
    "    # Return data structure with information\n",
    "    return {\n",
    "        'files':  n_fileids,\n",
    "        'topics': n_topics,\n",
    "        'paras':  counts['paras'],\n",
    "        'sents':  counts['sents'],\n",
    "        'words':  counts['words'],\n",
    "        'vocab':  len(tokens),\n",
    "        'lexdiv': float(counts['words']) / float(len(tokens)),\n",
    "        'ppdoc':  float(counts['paras']) / float(n_fileids),\n",
    "        'sppar':  float(counts['sents']) / float(counts['paras']),\n",
    "        'secs':   time.time() - started,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    The preprocessor wraps an `HTMLCorpusReader` and performs tokenization\n",
    "    and part-of-speech tagging.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, target=None, **kwargs):\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "\n",
    "    def fileids(self, fileids=None, categories=None):\n",
    "        fileids = self.corpus.resolve(fileids, categories)\n",
    "        if fileids:\n",
    "            return fileids\n",
    "        return self.corpus.fileids()\n",
    "\n",
    "    def abspath(self, fileid):\n",
    "        # Find the directory, relative to the corpus root.\n",
    "        parent = os.path.relpath(\n",
    "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root\n",
    "        )\n",
    "\n",
    "        # Compute the name parts to reconstruct\n",
    "        basename  = os.path.basename(fileid)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "\n",
    "        # Create the pickle file extension\n",
    "        basename  = name + '.pickle'\n",
    "\n",
    "        # Return the path to the file relative to the target.\n",
    "        return os.path.normpath(os.path.join(self.target, parent, basename))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "\n",
    "def tokenize(self, fileid):\n",
    "    for paragraph in self.corpus.paras(fileids=fileid):\n",
    "        yield [\n",
    "            pos_tag(wordpunct_tokenize(sent))\n",
    "            for sent in sent_tokenize(paragraph)\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def process(self, fileid):\n",
    "    \"\"\"\n",
    "    For a single file, checks the location on disk to ensure no errors,\n",
    "    uses +tokenize()+ to perform the preprocessing, and writes transformed\n",
    "    document as a pickle to target location.\n",
    "    \"\"\"\n",
    "    # Compute the outpath to write the file to.\n",
    "    target = self.abspath(fileid)\n",
    "    parent = os.path.dirname(target)\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    if not os.path.exists(parent):\n",
    "        os.makedirs(parent)\n",
    "\n",
    "    # Make sure that the parent is a directory and not a file\n",
    "    if not os.path.isdir(parent):\n",
    "        raise ValueError(\n",
    "            \"Please supply a directory to write preprocessed data to.\"\n",
    "        )\n",
    "\n",
    "    # Create a data structure for the pickle\n",
    "    document = list(self.tokenize(fileid))\n",
    "\n",
    "    # Open and serialize the pickle to disk\n",
    "    with open(target, 'wb') as f:\n",
    "        pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Clean up the document\n",
    "    del document\n",
    "    # Return the target fileid\n",
    "    return target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def transform(self, fileids=None, categories=None):\n",
    "    # Make the target directory if it doesn't already exist\n",
    "    if not os.path.exists(self.target):\n",
    "        os.makedirs(self.target)\n",
    "\n",
    "    # Resolve the fileids to start processing\n",
    "    for fileid in self.fileids(fileids, categories):\n",
    "        yield self.process(fileid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
    "\n",
    "class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for raw HTML documents to enable preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, fileids=DOC_PATTERN, encoding='utf8',\n",
    "                 tags=TAGS, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining\n",
    "        arguments are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        # Initialize the NLTK corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "\n",
    "        # Save the tags that we specifically want to extract.\n",
    "        self.tags = tags"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "\n",
    "class PickledCorpusReader(HTMLCorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        # Load one pickled document into memory at a time.\n",
    "        for path in self.abspaths(fileids):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def paras(self, fileids=None, categories=None):\n",
    "    for doc in self.docs(fileids, categories):\n",
    "        for para in doc:\n",
    "            yield para"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def sents(self, fileids=None, categories=None):\n",
    "    for para in self.paras(fileids, categories):\n",
    "        for sent in para:\n",
    "            yield sent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def tagged(self, fileids=None, categories=None):\n",
    "    for sent in self.sents(fileids, categories):\n",
    "        for tagged_token in sent:\n",
    "            yield tagged_token\n",
    "\n",
    "def words(self, fileids=None, categories=None):\n",
    "    for tagged in self.tagged(fileids, categories):\n",
    "        yield tagged[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}