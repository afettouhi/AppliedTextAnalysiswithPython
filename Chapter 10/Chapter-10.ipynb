{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "\n",
    "class Dialog(abc.ABC):\n",
    "    \"\"\"\n",
    "    A dialog listens for utterances, parses and interprets them, then updates\n",
    "    its internal state. It can then formulate a response on demand.\n",
    "    \"\"\"\n",
    "\n",
    "    def listen(self, text, response=True, **kwargs):\n",
    "        \"\"\"\n",
    "        A text utterance is passed in and parsed. It is then passed to the\n",
    "        interpret method to determine how to respond. If a response is\n",
    "        requested, the respond method is used to generate a text response\n",
    "        based on the most recent input and the current Dialog state.\n",
    "        \"\"\"\n",
    "        # Parse the input\n",
    "        sents = self.parse(text)\n",
    "\n",
    "        # Interpret the input\n",
    "        sents, confidence, kwargs = self.interpret(sents, **kwargs)\n",
    "\n",
    "        # Determine the response\n",
    "        if response:\n",
    "            reply = self.respond(sents, confidence, **kwargs)\n",
    "        else:\n",
    "            reply = None\n",
    "\n",
    "        # Return initiative\n",
    "        return reply, confidence\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        Every dialog may need its own parsing strategy, some dialogs may need\n",
    "        dependency vs. constituency parses, others may simply require regular\n",
    "        expressions or chunkers.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def interpret(self, sents, **kwargs):\n",
    "        \"\"\"\n",
    "        Interprets the utterance passed in as a list of parsed sentences,\n",
    "        updates the internal state of the dialog, computes a confidence of the\n",
    "        interpretation. May also return arguments specific to the response\n",
    "        mechanism.\n",
    "        \"\"\"\n",
    "        return sents, 0.0, kwargs\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def respond(self, sents, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a response given the input utterances and the current state of\n",
    "        the dialog, along with any arguments passed in from the listen or the\n",
    "        interpret methods.\n",
    "        \"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "class SimpleConversation(Dialog, Sequence):\n",
    "    \"\"\"\n",
    "    This is the most simple version of a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dialogs):\n",
    "        self._dialogs = dialogs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._dialogs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dialogs)\n",
    "\n",
    "    def listen(self, text, response=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Simply return the best confidence response\n",
    "        \"\"\"\n",
    "        responses = [\n",
    "            dialog.listen(text, response, **kwargs)\n",
    "            for dialog in self._dialogs\n",
    "        ]\n",
    "\n",
    "        # Responses is a list of (response, confidence) pairs\n",
    "        return max(responses, key=itemgetter(1))\n",
    "\n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        Returns parses for all internal dialogs for debugging\n",
    "        \"\"\"\n",
    "        return [dialog.parse(text) for dialog in self._dialogs]\n",
    "\n",
    "    def interpret(self, sents, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns interpretations for all internal dialogs for debugging\n",
    "        \"\"\"\n",
    "        return [dialog.interpret(sents, **kwargs) for dialog in self._dialogs]\n",
    "\n",
    "    def respond(self, sents, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns responses for all internal dialogs for debugging\n",
    "        \"\"\"\n",
    "        return [\n",
    "            dialog.respond(sents, confidence, **kwargs)\n",
    "            for dialog in self._dialogs\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Greeting(Dialog):\n",
    "    \"\"\"\n",
    "    Keeps track of the participants entering or leaving the conversation and\n",
    "    responds with appropriate salutations. This is an example of a rules based\n",
    "    system that keeps track of state and uses regular expressions and logic to\n",
    "    handle the dialog.\n",
    "    \"\"\"\n",
    "\n",
    "    PATTERNS = {\n",
    "        'greeting': r'hello|hi|hey|good morning|good evening',\n",
    "        'introduction': r'my name is ([a-z\\-\\s]+)',\n",
    "        'goodbye': r'goodbye|bye|ttyl',\n",
    "        'rollcall': r'roll call|who\\'s here?',\n",
    "    }\n",
    "\n",
    "    def __init__(self, participants=None):\n",
    "        # Participants is a map of user name to real name\n",
    "        self.participants = {}\n",
    "\n",
    "        if participants is not None:\n",
    "            for participant in participants:\n",
    "                self.participants[participant] = None\n",
    "\n",
    "        # Compile regular expressions\n",
    "        self._patterns = {\n",
    "            key: re.compile(pattern, re.I)\n",
    "            for key, pattern in self.PATTERNS.items()\n",
    "        }\n",
    "\n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        Applies all regular expressions to the text to find matches.\n",
    "        \"\"\"\n",
    "        matches = {}\n",
    "        for key, pattern in self._patterns.items():\n",
    "            match = pattern.match(text)\n",
    "            if match is not None:\n",
    "                matches[key] = match\n",
    "        return matches\n",
    "\n",
    "    def interpret(self, sents, **kwargs):\n",
    "        \"\"\"\n",
    "        Takes in parsed matches and determines if the message is an enter,\n",
    "        exit, or name change.\n",
    "        \"\"\"\n",
    "        # Can't do anything with no matches\n",
    "        if len(sents) == 0:\n",
    "            return sents, 0.0, kwargs\n",
    "\n",
    "        # Get username from the participants\n",
    "        user = kwargs.get('user', None)\n",
    "\n",
    "        # Determine if an introduction has been made\n",
    "        if 'introduction' in sents:\n",
    "            # Get the name from the utterance\n",
    "            name = sents['introduction'].groups()[0]\n",
    "            user = user or name.lower()\n",
    "\n",
    "            # Determine if name has changed\n",
    "            if user not in self.participants or self.participants[user] != name:\n",
    "                kwargs['name_changed'] = True\n",
    "\n",
    "            # Update the participants\n",
    "            self.participants[user] = name\n",
    "            kwargs['user'] = user\n",
    "\n",
    "        # Determine if a greeting has been made\n",
    "        if 'greeting' in sents:\n",
    "            # If we don't have a name for the user\n",
    "            if not self.participants.get(user, None):\n",
    "                kwargs['request_introduction'] = True\n",
    "\n",
    "        # Determine if goodbye has been made\n",
    "        if 'goodbye' in sents and user is not None:\n",
    "            # Remove participant\n",
    "            self.participants.pop(user)\n",
    "            kwargs.pop('user', None)\n",
    "\n",
    "        # If we've seen anything we're looking for, we're pretty confident\n",
    "        return sents, 1.0, kwargs\n",
    "\n",
    "    def respond(self, sents, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Gives a greeting or a goodbye depending on what's appropriate.\n",
    "        \"\"\"\n",
    "        if confidence == 0:\n",
    "            return None\n",
    "\n",
    "        name = self.participants.get(kwargs.get('user', None), None)\n",
    "        name_changed = kwargs.get('name_changed', False)\n",
    "        request_introduction = kwargs.get('request_introduction', False)\n",
    "\n",
    "        if 'greeting' in sents or 'introduction' in sents:\n",
    "            if request_introduction:\n",
    "                return \"Hello, what is your name?\"\n",
    "            else:\n",
    "                return \"Hello, {}!\".format(name)\n",
    "\n",
    "        if 'goodbye' in sents:\n",
    "            return \"Talk to you later!\"\n",
    "\n",
    "        if 'rollcall' in sents:\n",
    "            people = list(self.participants.values())\n",
    "\n",
    "            if len(people) > 1:\n",
    "                roster = \", \".join(people[:-1])\n",
    "                roster += \" and {}.\".format(people[-1])\n",
    "                return \"Currently in the conversation are \" + roster\n",
    "\n",
    "            elif len(people) == 1:\n",
    "                return \"It's just you and me right now, {}.\".format(name)\n",
    "            else:\n",
    "                return \"So lonely in here by myself ... wait who is that?\"\n",
    "\n",
    "        raise Exception(\n",
    "            \"expected response to be returned, but could not find rule\"\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, what is your name?\n",
      "Hello, Jake!\n",
      "It's just you and me right now, Jake.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dialog = Greeting()\n",
    "    # `listen` returns (response, confidence) tuples; just print the response\n",
    "    print(dialog.listen(\"Hello!\", user=\"jakevp321\")[0])\n",
    "    print(dialog.listen(\"my name is Jake\", user=\"jakevp321\")[0])\n",
    "    print(dialog.listen(\"Roll call!\", user=\"jakevp321\")[0])\n",
    "    print(dialog.listen(\"Have to go, goodbye!\", user=\"jakevp321\")[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, what is your name?\n",
      "Hello, Jill!\n",
      "It's just you and me right now, None.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dialog = Greeting()\n",
    "    print(dialog.listen(\"hey\", user=\"jillmonger\")[0])\n",
    "    print(dialog.listen(\"my name is Jill.\", user=\"jillmonger\")[0])\n",
    "    print(dialog.listen(\"who's here?\")[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "class TestBaseClasses(object):\n",
    "    \"\"\"\n",
    "    Tests for the Dialog class\n",
    "    \"\"\"\n",
    "    @pytest.mark.parametrize(\"text\", [\n",
    "        \"Gobbledeguk\", \"Gibberish\", \"Wingdings\"\n",
    "    ])\n",
    "    def test_dialog_abc(self, text):\n",
    "        \"\"\"\n",
    "        Test the Dialog ABC and the listen method\n",
    "        \"\"\"\n",
    "        class SampleDialog(Dialog):\n",
    "            def parse(self, text):\n",
    "                return []\n",
    "\n",
    "            def interpret(self, sents):\n",
    "                return sents, 0.0, {}\n",
    "\n",
    "            def respond(self, sents, confidence):\n",
    "                return None\n",
    "\n",
    "        sample = SampleDialog()\n",
    "        reply, confidence = sample.listen(text)\n",
    "        assert confidence == 0.0\n",
    "        assert reply is None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class TestGreetingDialog(object):\n",
    "    \"\"\"\n",
    "    Test expected input and responses for the Greeting dialog\n",
    "    \"\"\"\n",
    "\n",
    "    @pytest.mark.parametrize(\"text\", [\"Hello!\", \"hello\", 'hey', 'hi'])\n",
    "    @pytest.mark.parametrize(\"user\", [None, \"jay\"], ids=[\"w/ user\", \"w/o user\"])\n",
    "    def test_greeting_intro(self, user, text):\n",
    "        \"\"\"\n",
    "        Test that an initial greeting requests an introduction\n",
    "        \"\"\"\n",
    "        g = Greeting()\n",
    "        reply, confidence = g.listen(text, user=user)\n",
    "        assert confidence == 1.0\n",
    "        assert reply is not None\n",
    "        assert reply == \"Hello, what is your name?\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "    @pytest.mark.xfail(reason=\"a case that must be handled\")\n",
    "    @pytest.mark.parametrize(\"text\", [\"My name is Jake\", \"Hello, I'm Jake.\"])\n",
    "    @pytest.mark.parametrize(\"user\", [None, \"jkm\"], ids=[\"w/ user\", \"w/o user\"])\n",
    "    def test_initial_intro(self, user, text):\n",
    "        \"\"\"\n",
    "        Test an initial introduction without greeting\n",
    "        \"\"\"\n",
    "        g = Greeting()\n",
    "        reply, confidence = g.listen(text, user=user)\n",
    "\n",
    "        assert confidence == 1.0\n",
    "        assert reply is not None\n",
    "        assert reply == \"Hello, Jake!\"\n",
    "\n",
    "        if user is None:\n",
    "            user = 'jake'\n",
    "\n",
    "        assert user in g.participants\n",
    "        assert g.participants[user] == 'Jake'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Required first: python -m spacy download en\n",
    "\n",
    "spacy_nlp = spacy.load(\"en\")\n",
    "\n",
    "def plot_displacy_tree(sent):\n",
    "    doc = spacy_nlp(sent)\n",
    "    displacy.serve(doc, style='dep')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-22ee4231a2e0>:3: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001B[91mnltk.parse.corenlp.CoreNLPParser\u001B[0m instead.\n",
      "  stanford_parser = StanfordParser(\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\n  NLTK was unable to find stanford-parser\\.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser\\.jar, see:\n    <https://nlp.stanford.edu/software/lex-parser.shtml>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-22ee4231a2e0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstanford\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mStanfordParser\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m stanford_parser = StanfordParser(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mmodel_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m )\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/nltk/parse/stanford.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    336\u001B[0m         )\n\u001B[1;32m    337\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 338\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mStanfordParser\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    339\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    340\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_make_tree\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/nltk/parse/stanford.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m         \u001B[0;31m# find the most recent code and model jar\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m         stanford_jar = max(\n\u001B[0m\u001B[1;32m     54\u001B[0m             find_jar_iter(\n\u001B[1;32m     55\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_JAR\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/nltk/internals.py\u001B[0m in \u001B[0;36mfind_jar_iter\u001B[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001B[0m\n\u001B[1;32m    831\u001B[0m             )\n\u001B[1;32m    832\u001B[0m         \u001B[0mdiv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"=\"\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m75\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 833\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\n\\n%s\\n%s\\n%s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdiv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmsg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdiv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    834\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    835\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mLookupError\u001B[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-parser\\.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser\\.jar, see:\n    <https://nlp.stanford.edu/software/lex-parser.shtml>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "stanford_parser = StanfordParser(\n",
    "    model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\"\n",
    ")\n",
    "def print_stanford_tree(sent):\n",
    "    \"\"\"\n",
    "    Use Stanford pretrained model to extract dependency tree\n",
    "    for use by other methods\n",
    "    Returns a list of trees\n",
    "    \"\"\"\n",
    "    parse = stanford_parser.raw_parse(sent)\n",
    "    return list(parse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def plot_stanford_tree(sent):\n",
    "    \"\"\"\n",
    "    Visually inspect the Stanford dependency tree as an image\n",
    "    \"\"\"\n",
    "    parse = stanford_parser.raw_parse(sent)\n",
    "    tree = list(parse)\n",
    "    tree[0].draw()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_stanford_tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-1fcf4629e64c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtree\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprint_stanford_tree\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"How many teaspoons are in a tablespoon?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mroot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtree\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m# The root is the first item in the parsed sents tree\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpos\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'print_stanford_tree' is not defined"
     ]
    }
   ],
   "source": [
    "tree = print_stanford_tree(\"How many teaspoons are in a tablespoon?\")\n",
    "root = tree[0] # The root is the first item in the parsed sents tree\n",
    "print(root)\n",
    "print(root.pos())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (<ipython-input-15-e8973bd372c2>, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-15-e8973bd372c2>\"\u001B[0;36m, line \u001B[0;32m66\u001B[0m\n\u001B[0;31m    if results[\"dst\"] in self.metrics[results[\"src\"]]):\u001B[0m\n\u001B[0m                                                     ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import inflect\n",
    "import humanize\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tree import Tree\n",
    "from nltk.util import breadth_first\n",
    "\n",
    "\n",
    "class Converter(Dialog):\n",
    "    \"\"\"\n",
    "    Answers questions about converting units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conversion_path=CONVERSION_PATH):\n",
    "        with open(conversion_path, 'r') as f:\n",
    "            self.metrics = json.load(f)\n",
    "\n",
    "        self.inflect = inflect.engine()\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.parser = StanfordParser(model_path=STANFORD_PATH)\n",
    "\n",
    "    def parse(self, text):\n",
    "        parse = self.parser.raw_parse(text)\n",
    "        return list(parse)\n",
    "\n",
    "    def interpret(self, sents, **kwargs):\n",
    "        measures = []\n",
    "        confidence = 0\n",
    "        results = dict()\n",
    "\n",
    "        # The root is the first item in the parsed sents tree\n",
    "        root = sents[0]\n",
    "\n",
    "        # Make sure there are wh-adverb phrases\n",
    "        if \"WRB\" in [tag for word, tag in root.pos()]:\n",
    "            # If so, increment confidence & traverse parse tree\n",
    "            confidence += .2\n",
    "            # Set the maxdepth to limit recursion\n",
    "            for clause in breadth_first(root, maxdepth=8):\n",
    "                #find the simple declarative clauses (+S+)\n",
    "                if isinstance(clause, Tree):\n",
    "                    if clause.label() in [\"S\", \"SQ\", \"WHNP\"]:\n",
    "                        for token,tag in clause.pos():\n",
    "                            # Store nouns as target measures\n",
    "                            if tag in [\"NN\", \"NNS\"]:\n",
    "                                measures.append(token)\n",
    "                            # Store numbers as target quantities\n",
    "                            elif tag in [\"CD\"]:\n",
    "                                results[\"quantity\"] = token\n",
    "\n",
    "            # Handle duplication for very nested trees\n",
    "            measures = list(set([self.stemmer.stem(mnt) for mnt in measures]))\n",
    "\n",
    "            # If both source and destination measures are provided...\n",
    "            if len(measures) == 2:\n",
    "                confidence += .4\n",
    "                results[\"src\"] = measures[0]\n",
    "                results[\"dst\"] = measures[1]\n",
    "\n",
    "                # Check to see if they correspond to our lookup table\n",
    "                if results[\"src\"] in self.metrics.keys():\n",
    "                    confidence += .2\n",
    "                    if results[\"dst\"] in self.metrics[results[\"src\"]]):\n",
    "                        confidence += .2\n",
    "\n",
    "        return results, confidence, kwargs\n",
    "\n",
    "    def convert(self, src, dst, quantity=1.0):\n",
    "        \"\"\"\n",
    "        Converts from the source unit to the dest unit for the given quantity\n",
    "        of the source unit.\n",
    "        \"\"\"\n",
    "        # Stem source and dest to remove pluralization\n",
    "        src, dst = tuple(map(self.stemmer.stem, (src,dst)))\n",
    "\n",
    "        # Check that we can convert\n",
    "        if dst not in self.metrics:\n",
    "            raise KeyError(\"cannot convert to '{}' units\".format(src))\n",
    "        if src not in self.metrics[dst]:\n",
    "            raise KeyError(\"cannot convert from {} to '{}'\".format(src, dst))\n",
    "\n",
    "        return self.metrics[dst][src] * float(quantity), src, dst\n",
    "\n",
    "    def round(self, num):\n",
    "        num = round(float(num), 4)\n",
    "        if num.is_integer():\n",
    "            return int(num)\n",
    "        return num\n",
    "\n",
    "    def pluralize(self, noun, num):\n",
    "        return self.inflect.plural_noun(noun, num)\n",
    "\n",
    "    def numericalize(self, amt):\n",
    "        if amt > 100.0 and amt < 1e6:\n",
    "            return humanize.intcomma(int(amt))\n",
    "        if amt >= 1e6:\n",
    "            return humanize.intword(int(amt))\n",
    "        elif isinstance(amt, int) or amt.is_integer():\n",
    "            return humanize.apnumber(int(amt))\n",
    "        else:\n",
    "            return humanize.fractional(amt)\n",
    "\n",
    "    def respond(self, sents, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Response makes use of the humanize and inflect libraries to produce\n",
    "        much more human understandable results.\n",
    "        \"\"\"\n",
    "        if confidence < .5:\n",
    "            return \"I'm sorry, I don't know that one.\"\n",
    "\n",
    "        try:\n",
    "            quantity = sents.get('quantity', 1)\n",
    "            amount, source, target = self.convert(**sents)\n",
    "\n",
    "            # Perform numeric rounding\n",
    "            amount = self.round(amount)\n",
    "            quantity = self.round(quantity)\n",
    "\n",
    "            # Pluralize\n",
    "            source = self.pluralize(source, quantity)\n",
    "            target = self.pluralize(target, amount)\n",
    "            verb = self.inflect.plural_verb(\"is\", amount)\n",
    "\n",
    "            # Numericalize\n",
    "            quantity = self.numericalize(quantity)\n",
    "            amount = self.numericalize(amount)\n",
    "\n",
    "\n",
    "            return \"There {} {} {} in {} {}.\".format(\n",
    "                verb, amount, target, quantity, source\n",
    "            )\n",
    "\n",
    "        except KeyError as e:\n",
    "            return \"I'm sorry I {}\".format(str(e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Converter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-c3389f57d80d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"__main__\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mdialog\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mConverter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdialog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlisten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"How many cups are in a gallon?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdialog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlisten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"How many gallons are in 2 cups?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdialog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlisten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"How many tablespoons are in a cup?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Converter' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dialog = Converter()\n",
    "    print(dialog.listen(\"How many cups are in a gallon?\"))\n",
    "    print(dialog.listen(\"How many gallons are in 2 cups?\"))\n",
    "    print(dialog.listen(\"How many tablespoons are in a cup?\"))\n",
    "    print(dialog.listen(\"How many tablespoons are in 10 cups?\"))\n",
    "    print(dialog.listen(\"How many tablespoons are in a teaspoon?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "def titles(self, fileids=None, categories=None):\n",
    "    \"\"\"\n",
    "    Parse HTML to identify titles from the head tag.\n",
    "    \"\"\"\n",
    "    for doc in self.docs(fileids, categories):\n",
    "        soup = bs4.BeautifulSoup(doc, 'lxml')\n",
    "        try:\n",
    "            yield soup.title.text\n",
    "            soup.decompose()\n",
    "        except AttributeError as e:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-934f5c285df2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mBallTreeRecommender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mBaseEstimator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTransformerMixin\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m     \"\"\"\n\u001B[1;32m     12\u001B[0m     \u001B[0mGiven\u001B[0m \u001B[0minput\u001B[0m \u001B[0mterms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprovide\u001B[0m \u001B[0mk\u001B[0m \u001B[0mrecipe\u001B[0m \u001B[0mrecommendations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class BallTreeRecommender(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Given input terms, provide k recipe recommendations\n",
    "    \"\"\"\n",
    "    def __init__(self, k=3, **kwargs):\n",
    "        self.k = k\n",
    "        self.trans_path = \"svd.pkl\"\n",
    "        self.tree_path = \"tree.pkl\"\n",
    "        self.transformer = False\n",
    "        self.tree = None\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load a pickled transformer and tree from disk,\n",
    "        if they exist.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.trans_path):\n",
    "            self.transformer = joblib.load(open(self.trans_path, 'rb'))\n",
    "            self.tree = joblib.load(open(self.tree_path, 'rb'))\n",
    "        else:\n",
    "            self.transformer = False\n",
    "            self.tree = None\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        It takes a long time to fit, so just do it once!\n",
    "        \"\"\"\n",
    "        joblib.dump(self.transformer, open(self.trans_path, 'wb'))\n",
    "        joblib.dump(self.tree, open(self.tree_path, 'wb'))\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        if self.transformer == False:\n",
    "            self.transformer = Pipeline([\n",
    "                ('norm', TextNormalizer(minimum=50, maximum=200)),\n",
    "                ('transform', Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer()),\n",
    "                    ('svd', TruncatedSVD(n_components=200))\n",
    "                ])\n",
    "                 )\n",
    "            ])\n",
    "            self.lexicon = self.transformer.fit_transform(documents)\n",
    "            self.tree = BallTree(self.lexicon)\n",
    "            self.save()\n",
    "\n",
    "    def query(self, terms):\n",
    "        \"\"\"\n",
    "        Given input list of ingredient terms, return k closest matching recipes.\n",
    "        \"\"\"\n",
    "        vect_doc = self.transformer.named_steps['transform'].fit_transform(terms)\n",
    "        dists, inds = self.tree.query(vect_doc, k=self.k)\n",
    "        return inds[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BallTreeRecommender' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-6b6368df1e85>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mRecipeRecommender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDialog\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \"\"\"\n\u001B[1;32m      3\u001B[0m     \u001B[0mRecipe\u001B[0m \u001B[0mrecommender\u001B[0m \u001B[0mdialog\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \"\"\"\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-20-6b6368df1e85>\u001B[0m in \u001B[0;36mRecipeRecommender\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m     \"\"\"\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecipes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecommender\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mBallTreeRecommender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecipes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtitles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecommender\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrecommender\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'BallTreeRecommender' is not defined"
     ]
    }
   ],
   "source": [
    "class RecipeRecommender(Dialog):\n",
    "    \"\"\"\n",
    "    Recipe recommender dialog\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, recipes, recommender=BallTreeRecommender(k=3)):\n",
    "        self.recipes = list(corpus.titles())\n",
    "        self.recommender = recommender\n",
    "        # Fit the recommender model with the corpus\n",
    "        self.recommender.fit_transform(list(corpus.docs()))\n",
    "\n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        Extract ingredients from the text\n",
    "        \"\"\"\n",
    "        return pos_tag(wordpunct_tokenize(text))\n",
    "\n",
    "    def interpret(self, sents, **kwargs):\n",
    "        # If feedback detected, update the model\n",
    "        if 'feedback' in kwargs:\n",
    "            self.recommender.update(kwargs['feedback'])\n",
    "\n",
    "        n_nouns = sum(1 for pos, tag in sents if pos.startswith(\"N\"))\n",
    "        confidence = n_nouns/len(sents)\n",
    "\n",
    "        terms = [tag for pos, tag in sents if pos.startswith(\"N\")]\n",
    "        return terms, confidence, kwargs\n",
    "\n",
    "    def respond(self, terms, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns a recommendation if the confidence is > 0.15 otherwise None.\n",
    "        \"\"\"\n",
    "        if confidence < 0.15:\n",
    "            return None\n",
    "\n",
    "        output = [\n",
    "            \"Here are some recipes related to {}\".format(\", \".join(terms))\n",
    "        ]\n",
    "        output += [\n",
    "            \"- {}\".format(self.recipes[idx])\n",
    "            for idx in self.recommender.query(terms)\n",
    "        ]\n",
    "\n",
    "        return \"\\n\".join(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTMLPickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-4515df65b5ae>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'__main__'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mHTMLPickledCorpusReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../food_corpus_proc'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mrecommender\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mRecipeRecommender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mquestion\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"What can I make with brie, tomatoes, capers, and pancetta?\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecommender\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlisten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquestion\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'HTMLPickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    corpus = HTMLPickledCorpusReader('../food_corpus_proc')\n",
    "    recommender = RecipeRecommender(corpus)\n",
    "    question = \"What can I make with brie, tomatoes, capers, and pancetta?\"\n",
    "    print(recommender.listen(question))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}