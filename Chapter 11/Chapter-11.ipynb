{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextNormalizer, identity\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def fit_naive_bayes(path, saveto=None, cv=12):\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    if saveto is None:\n",
    "        saveto = \"naive_bayes_{}.pkl\".format(time.time())\n",
    "\n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"naive bayes training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))\n",
    "\n",
    "\n",
    "def fit_logistic_regression(path, saveto=None, cv=12):\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    if saveto is None:\n",
    "        saveto = \"logistic_regression_{}.pkl\".format(time.time())\n",
    "\n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"logistic regression training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))\n",
    "\n",
    "\n",
    "def fit_multilayer_perceptron(path, saveto=None, cv=12):\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=(10,10), early_stopping=True))\n",
    "    ])\n",
    "\n",
    "    if saveto is None:\n",
    "        saveto = \"multilayer_perceptron_{}.pkl\".format(time.time())\n",
    "\n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"multilayer perceptron training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-8e722bf8a7cf>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;34m@\u001B[0m\u001B[0mtimeit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msaveto\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;31m# Load the corpus data and labels for classification\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "from reader import PickledCorpusReader\n",
    "\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "@timeit\n",
    "def train_model(path, model, saveto=None, cv=12):\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = PickledCorpusReader(path)\n",
    "    X = documents(corpus)\n",
    "    y = labels(corpus)\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv)\n",
    "\n",
    "    # Fit the model on entire dataset\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "    # Return scores as well as training time via decorator\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def documents(corpus):\n",
    "    return [\n",
    "        list(corpus.docs(fileids=fileid))\n",
    "        for fileid in corpus.fileids()\n",
    "    ]\n",
    "\n",
    "\n",
    "def labels(corpus):\n",
    "    return [\n",
    "        corpus.categories(fileids=fileid)[0]\n",
    "        for fileid in corpus.fileids()\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        return result, time.time() - start\n",
    "    return wrapper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(processName)-10s %(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess 2020-08-14 05:09:00 beginning parallel tasks\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-6e1a4fedcc7a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'__main__'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0mrun_parallel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"corpus/\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-6e1a4fedcc7a>\u001B[0m in \u001B[0;36mrun_parallel\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mprocs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mtask\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         \u001B[0mproc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mProcess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0mprocs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mproc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mproc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mp' is not defined"
     ]
    }
   ],
   "source": [
    "def run_parallel(path):\n",
    "    tasks = [\n",
    "        fit_naive_bayes, fit_logistic_regression, fit_multilayer_perceptron,\n",
    "    ]\n",
    "\n",
    "    logger.info(\"beginning parallel tasks\")\n",
    "    start = time.time()\n",
    "\n",
    "    procs = []\n",
    "    for task in tasks:\n",
    "        proc = mp.Process(name=task.__name__, target=task, args=(path,))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "\n",
    "    delta = time.time() - start\n",
    "    logger.info(\"total parallel fit time: {:0.2f} seconds\".format(delta))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_parallel(\"corpus/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-f0c4d519b81f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mParallelPreprocessor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPreprocessor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mon_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "class ParallelPreprocessor(Preprocessor):\n",
    "\n",
    "    def on_result(self, result):\n",
    "        self.results.append(result)\n",
    "\n",
    "    def transform(self, tasks=None):\n",
    "        [...]\n",
    "\n",
    "        # Reset the results\n",
    "        self.results = []\n",
    "\n",
    "        # Create a multiprocessing pool\n",
    "        tasks = tasks or mp.cpu_count()\n",
    "        pool  = mp.Pool(processes=tasks)\n",
    "\n",
    "        # Enqueue tasks on the multiprocessing pool and join\n",
    "        for fileid in self.fileids():\n",
    "            pool.apply_async(\n",
    "                self.process, (fileid,), callback=self.on_result\n",
    "            )\n",
    "\n",
    "        # Close the pool and join\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        return self.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-f7af06e2aa9c>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-9-f7af06e2aa9c>\"\u001B[0;36m, line \u001B[0;32m10\u001B[0m\n\u001B[0;31m    if __name__ == \"__main__\":\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "APP_NAME = \"My Spark Application\"\n",
    "\n",
    "\n",
    "def main(sc):\n",
    "    # Define RDDs and apply operations and actions to them.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    sc   = SparkContext(conf=conf)\n",
    "\n",
    "    # Execute Main functionality\n",
    "    main(sc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-e27147c9d3a3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwholeTextFiles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"hobbies/*/*.txt\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = sc.wholeTextFiles(\"hobbies/*/*.txt\")\n",
    "print(corpus.take(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-22f8acd59b90>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwholeTextFiles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"corpus/*.jsonl\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m corpus = corpus.flatMap(\n\u001B[1;32m      5\u001B[0m     lambda d: [\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "corpus = sc.wholeTextFiles(\"corpus/*.jsonl\")\n",
    "corpus = corpus.flatMap(\n",
    "    lambda d: [\n",
    "        json.loads(line)\n",
    "        for line in d[1].split(\"\\n\")\n",
    "        if line\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-c79c5fa9a4a0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwholeTextFiles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"hobbies/*/*.txt\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitemgetter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparse_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "def parse_label(path):\n",
    "    # Returns the name of the directory containing the file\n",
    "    return os.path.basename(os.path.dirname(path))\n",
    "\n",
    "\n",
    "data = sc.wholeTextFiles(\"hobbies/*/*.txt\")\n",
    "labels = data.map(itemgetter(0)).map(parse_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-c5dd109d9dfd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0moperator\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0madd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mlabel_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduceByKey\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlabel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcount\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlabel_count\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"{}: {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcount\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "label_count = labels.map(lambda l: (l, 1)).reduceByKey(add)\n",
    "for label, count in label_count.collect():\n",
    "    print(\"{}: {}\".format(label, count))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-587144e5995e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Load data from disk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwholeTextFiles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"hobbies/*/*.txt\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Parse the label from the text path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mparse_label\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data from disk\n",
    "corpus = sc.wholeTextFiles(\"hobbies/*/*.txt\")\n",
    "\n",
    "# Parse the label from the text path\n",
    "corpus = corpus.map(lambda d: (parse_label(d[0]), d[1]))\n",
    "\n",
    "# Create the dataframe with two columns\n",
    "df = spark.createDataFrame(corpus, [\"category\", \"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-15-3c37979073f2>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-15-3c37979073f2>\"\u001B[0;36m, line \u001B[0;32m11\u001B[0m\n\u001B[0;31m    if __name__ == \"__main__\":\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "APP_NAME = \"My Spark Text Analysis\"\n",
    "\n",
    "\n",
    "def main(sc, spark):\n",
    "    # Define DataFrames and apply ML estimators and transformers\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    sc   = SparkContext(conf=conf)\n",
    "\n",
    "    # Build SparkSQL Session\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "    # Execute Main functionality\n",
    "    main(sc, spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-99fb4f8e5198>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# Create the RegexTokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m tokens = RegexTokenizer(\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"text\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"tokens\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     pattern=\"\\\\w+\", gaps=False, toLowercase=True)\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/feature.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, minTokenLength, gaps, pattern, inputCol, outputCol, toLowercase)\u001B[0m\n\u001B[1;32m   3139\u001B[0m         \"\"\"\n\u001B[1;32m   3140\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mRegexTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3141\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new_java_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"org.apache.spark.ml.feature.RegexTokenizer\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3142\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_setDefault\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mminTokenLength\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgaps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpattern\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\\\\s+\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtoLowercase\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3143\u001B[0m         \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \"\"\"\n\u001B[1;32m     64\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m         \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_jvm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mjava_class\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_obj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/util.py\u001B[0m in \u001B[0;36m_jvm\u001B[0;34m()\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjvm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "# Create the RegexTokenizer\n",
    "tokens = RegexTokenizer(\n",
    "    inputCol=\"text\", outputCol=\"tokens\",\n",
    "    pattern=\"\\\\w+\", gaps=False, toLowercase=True)\n",
    "\n",
    "# Transform the corpus\n",
    "corpus = tokens.transform(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def make_vectorizer(stopwords=True, tfidf=True, n_features=5000):\n",
    "    # Creates a vectorization pipeline that starts with tokenization\n",
    "    stages = [\n",
    "        Tokenizer(inputCol=\"text\", outputCol=\"tokens\"),\n",
    "    ]\n",
    "\n",
    "    # Append stopwords to the pipeline if requested\n",
    "    if stopwords:\n",
    "        stages.append(\n",
    "            StopWordsRemover(\n",
    "                caseSensitive=False, outputCol=\"filtered_tokens\",\n",
    "                inputCol=stages[-1].getOutputCol(),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Create the Hashing term frequency vectorizer\n",
    "    stages.append(\n",
    "        HashingTF(\n",
    "            numFeatures=n_features,\n",
    "            inputCol=stages[-1].getOutputCol(),\n",
    "            outputCol=\"frequency\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Append the IDF vectorizer if requested\n",
    "    if tfidf:\n",
    "        stages.append(\n",
    "            IDF(inputCol=stages[-1].getOutputCol(), outputCol=\"tfidf\")\n",
    "        )\n",
    "\n",
    "    # Return the completed pipeline\n",
    "    return Pipeline(stages=stages)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-ebc42526d6f9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m# Create the vector/cluster pipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m pipeline = Pipeline(stages=[\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0mTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"text\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"tokens\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m     \u001B[0mWord2Vec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvectorSize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mminCount\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"tokens\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"vecs\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mBisectingKMeans\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeaturesCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"vecs\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxIter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/feature.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, inputCol, outputCol)\u001B[0m\n\u001B[1;32m   3973\u001B[0m         \"\"\"\n\u001B[1;32m   3974\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3975\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new_java_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"org.apache.spark.ml.feature.Tokenizer\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3976\u001B[0m         \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3977\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetParams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \"\"\"\n\u001B[1;32m     64\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m         \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_jvm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mjava_class\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_obj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/util.py\u001B[0m in \u001B[0;36m_jvm\u001B[0;34m()\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjvm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.feature import Word2Vec, Tokenizer\n",
    "\n",
    "# Create the vector/cluster pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"tokens\"),\n",
    "    Word2Vec(vectorSize=7, minCount=0, inputCol=\"tokens\", outputCol=\"vecs\"),\n",
    "    BisectingKMeans(k=10, featuresCol=\"vecs\", maxIter=10),\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(corpus)\n",
    "corpus = model.transform(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-bd7dd70ac901>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Retrieve stages\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mbkm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstages\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mwvec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstages\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# Evaluate clustering\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Retrieve stages\n",
    "bkm = model.stages[-1]\n",
    "wvec = model.stages[-2]\n",
    "\n",
    "# Evaluate clustering\n",
    "cost = bkm.computeCost(corpus)\n",
    "sizes = bkm.summary.clusterSizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bkm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-0bdea301db16>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Get the text representation of each cluster\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mtable\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Cluster\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Size\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Terms\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mci\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbkm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclusterCenters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwvec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfindSynonyms\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m7\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0msize\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msizes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mci\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'bkm' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the text representation of each cluster\n",
    "table = [[\"Cluster\", \"Size\", \"Terms\"]]\n",
    "for ci, c in enumerate(bkm.clusterCenters()):\n",
    "    ct = wvec.findSynonyms(c, 7)\n",
    "    size = sizes[ci]\n",
    "    terms = \" \".join([row.word for row in ct.take(7)])\n",
    "    table.append([ci, size, terms])\n",
    "\n",
    "# Print the results\n",
    "print(tabulate(table))\n",
    "print(\"Sum of square distance to center: {:0.3f}\".format(cost))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-b2ddf7552f55>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# Create the vectorizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mvector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmake_vectorizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m# Index the labels of the classification\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-17-92fa377760b5>\u001B[0m in \u001B[0;36mmake_vectorizer\u001B[0;34m(stopwords, tfidf, n_features)\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;31m# Creates a vectorization pipeline that starts with tokenization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     stages = [\n\u001B[0;32m----> 4\u001B[0;31m         \u001B[0mTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"text\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"tokens\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     ]\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/feature.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, inputCol, outputCol)\u001B[0m\n\u001B[1;32m   3973\u001B[0m         \"\"\"\n\u001B[1;32m   3974\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3975\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new_java_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"org.apache.spark.ml.feature.Tokenizer\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3976\u001B[0m         \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3977\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetParams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \"\"\"\n\u001B[1;32m     64\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m         \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_jvm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mjava_class\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_obj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/util.py\u001B[0m in \u001B[0;36m_jvm\u001B[0;34m()\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjvm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create the vectorizer\n",
    "vector = make_vectorizer().fit(corpus)\n",
    "\n",
    "# Index the labels of the classification\n",
    "labelIndex = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "labelIndex = labelIndex.fit(corpus)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "training, test = corpus.randomSplit([0.8, 0.2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-ee632ff0b701>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m model = Pipeline(stages=[\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mvector\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabelIndex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m ]).fit(training)\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'vector' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "model = Pipeline(stages=[\n",
    "    vector, labelIndex, clf\n",
    "]).fit(training)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"tfidf\").show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-3829e6e2b337>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluation\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mMulticlassClassificationEvaluator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m evaluator = MulticlassClassificationEvaluator(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mlabelCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"indexedLabel\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mpredictionCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"prediction\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/evaluation.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, predictionCol, labelCol, metricName, weightCol, metricLabel, beta, probabilityCol, eps)\u001B[0m\n\u001B[1;32m    441\u001B[0m         \"\"\"\n\u001B[1;32m    442\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMulticlassClassificationEvaluator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 443\u001B[0;31m         self._java_obj = self._new_java_obj(\n\u001B[0m\u001B[1;32m    444\u001B[0m             \"org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\", self.uid)\n\u001B[1;32m    445\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_setDefault\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmetricName\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"f1\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetricLabel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbeta\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1.0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \"\"\"\n\u001B[1;32m     64\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m         \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_jvm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mjava_class\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_obj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/util.py\u001B[0m in \u001B[0;36m_jvm\u001B[0;34m()\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjvm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "score = evaluator.evaluate(predictions)\n",
    "print(\"F1 Score: {:0.3f}\".format(score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-24-d7364cc94dd5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Vectorize the corpus on the cluster\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mvector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmake_vectorizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# Get the sample from the dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-17-92fa377760b5>\u001B[0m in \u001B[0;36mmake_vectorizer\u001B[0;34m(stopwords, tfidf, n_features)\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;31m# Creates a vectorization pipeline that starts with tokenization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     stages = [\n\u001B[0;32m----> 4\u001B[0;31m         \u001B[0mTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"text\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"tokens\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     ]\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/__init__.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Method %s forces keyword arguments.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/feature.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, inputCol, outputCol)\u001B[0m\n\u001B[1;32m   3973\u001B[0m         \"\"\"\n\u001B[1;32m   3974\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3975\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new_java_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"org.apache.spark.ml.feature.Tokenizer\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3976\u001B[0m         \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_kwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3977\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetParams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \"\"\"\n\u001B[1;32m     64\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m         \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_jvm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mjava_class\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_obj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/AppliedTextAnalysiswithPython/lib/python3.8/site-packages/pyspark/ml/util.py\u001B[0m in \u001B[0;36m_jvm\u001B[0;34m()\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjvm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "# Vectorize the corpus on the cluster\n",
    "vector = make_vectorizer().fit(corpus)\n",
    "corpus = vector.transform(corpus)\n",
    "\n",
    "# Get the sample from the dataset\n",
    "sample = corpus.sample(False, 0.1).collect()\n",
    "X = [row['tfidf'] for row in sample]\n",
    "y = [row['label'] for row in sample]\n",
    "\n",
    "# Train a Scikit-Learn Model\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-1fb8f686c70e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Broadcast the Scikit-Learn Model to the cluster\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mclf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbroadcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Create accumulators for correct vs incorrect\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mcorrect\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maccumulator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# Broadcast the Scikit-Learn Model to the cluster\n",
    "clf = sc.broadcast(clf)\n",
    "\n",
    "# Create accumulators for correct vs incorrect\n",
    "correct = sc.accumulator(0)\n",
    "incorrect = sc.accumulator(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def make_accuracy_closure(model, correct, incorrect):\n",
    "    # model should be a broadcast variable\n",
    "    # correct and incorrect should be accumulators\n",
    "    def inner(rows):\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for row in rows:\n",
    "            X.append(row['tfidf'])\n",
    "            y.append(row['label'])\n",
    "\n",
    "        yp = model.value.predict(X)\n",
    "        for yi, ypi in zip(y, yp):\n",
    "            if yi == ypi:\n",
    "                correct.add(1)\n",
    "            else:\n",
    "                incorrect.add(1)\n",
    "    return inner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-0db8a420a2d6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Create the accuracy closure\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0maccuracy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmake_accuracy_closure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mincorrect\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorrect\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Compute the number incorrect and correct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforeachPartition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maccuracy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the accuracy closure\n",
    "accuracy = make_accuracy_closure(clf, incorrect, correct)\n",
    "\n",
    "# Compute the number incorrect and correct\n",
    "corpus.foreachPartition(accuracy)\n",
    "\n",
    "accuracy = float(correct.value) / float(correct.value + incorrect.value)\n",
    "print(\"Global accuracy of model was {}\".format(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}