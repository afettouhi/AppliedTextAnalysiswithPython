{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KMeansClusters(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, k=7):\n",
    "        \"\"\"\n",
    "        k is the number of clusters\n",
    "        model is the implementation of Kmeans\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distance = nltk.cluster.util.cosine_distance\n",
    "        self.model = KMeansClusterer(self.k, self.distance,\n",
    "                                     avoid_empty_clusters=True)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Fits the K-Means model to one-hot vectorized documents.\n",
    "        \"\"\"\n",
    "        return self.model.cluster(documents, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class OneHotVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        freqs = self.vectorizer.fit_transform(documents)\n",
    "        return [freq.toarray()[0] for freq in freqs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-d1070e8ac3f9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipeline\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPickledCorpusReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/sample'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mdocs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcategories\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'news'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "corpus = PickledCorpusReader('../data/sample')\n",
    "docs = corpus.docs(categories=['news'])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('vect', OneHotVectorizer()),\n",
    "    ('clusters', KMeansClusters(k=7))\n",
    "])\n",
    "\n",
    "clusters = model.fit_transform(docs)\n",
    "pickles = list(corpus.fileids(categories=['news']))\n",
    "for idx, cluster in enumerate(clusters):\n",
    "    print(\"Document '{}' assigned to cluster {}.\".format(pickles[idx],cluster))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class KMeansClusters(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, k=7):\n",
    "        self.k = k\n",
    "        self.model = MiniBatchKMeans(self.k)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return self.model.fit_predict(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "class HierarchicalClusters(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = AgglomerativeClustering()\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Fits the agglomerative model to the given data.\n",
    "        \"\"\"\n",
    "        clusters = self.model.fit_predict(documents)\n",
    "        self.labels = self.model.labels_\n",
    "        self.children = self.model.children_\n",
    "\n",
    "        return clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextNormalizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-490f90db8f1c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m model = Pipeline([\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0;34m(\u001B[0m\u001B[0;34m'norm'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTextNormalizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;34m(\u001B[0m\u001B[0;34m'vect'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mOneHotVectorizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;34m(\u001B[0m\u001B[0;34m'clusters'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mHierarchicalClusters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m ])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'TextNormalizer' is not defined"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('vect', OneHotVectorizer()),\n",
    "    ('clusters', HierarchicalClusters())\n",
    "])\n",
    "\n",
    "model.fit_transform(docs)\n",
    "labels = model.named_steps['clusters'].labels\n",
    "pickles = list(corpus.fileids(categories=['news']))\n",
    "\n",
    "for idx, fileid in enumerate(pickles):\n",
    "    print(\"Document '{}' assigned to cluster {}.\".format(fileid,labels[idx]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-8f1841034fb8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtight_layout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m \u001B[0mchildren\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnamed_steps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'clusters'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m \u001B[0mplot_dendrogram\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchildren\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(children, **kwargs):\n",
    "    # Distances between each pair of children\n",
    "    distance = position = np.arange(children.shape[0])\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([\n",
    "        children, distance, position]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))  # set size\n",
    "    ax = dendrogram(linkage_matrix, **kwargs)\n",
    "    plt.tick_params(axis='x', bottom='off', top='off', labelbottom='off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "children = model.named_steps['clusters'].children\n",
    "plot_dendrogram(children)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SklearnTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('vect', CountVectorizer(tokenizer=identity,\n",
    "                                     preprocessor=None, lowercase=False)),\n",
    "            ('model', LatentDirichletAllocation(n_topics=self.n_topics)),\n",
    "        ])\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.model.fit_transform(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def get_topics(self, n=25):\n",
    "        \"\"\"\n",
    "        n is the number of top terms to show for each topic\n",
    "        \"\"\"\n",
    "        vectorizer = self.model.named_steps['vect']\n",
    "        model = self.model.steps[-1][1]\n",
    "        names = vectorizer.get_feature_names()\n",
    "        topics = dict()\n",
    "\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            features = topic.argsort()[:-(n - 1): -1]\n",
    "            tokens = [names[i] for i in features]\n",
    "            topics[idx] = tokens\n",
    "\n",
    "        return topics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-084f8480d3b5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'__main__'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPickledCorpusReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/sample'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mlda\u001B[0m       \u001B[0;34m=\u001B[0m \u001B[0mSklearnTopicModels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mdocuments\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    corpus = PickledCorpusReader('../data/sample')\n",
    "\n",
    "    lda       = SklearnTopicModels()\n",
    "    documents = corpus.docs()\n",
    "\n",
    "    lda.fit_transform(documents)\n",
    "    topics = lda.get_topics()\n",
    "    for topic, terms in topics.items():\n",
    "        print(\"Topic #{}:\".format(topic+1))\n",
    "        print(terms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class GensimTfidfVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dirpath=\".\", tofull=False):\n",
    "        \"\"\"\n",
    "        Pass in a directory that holds the lexicon in corpus.dict and the\n",
    "        TF-IDF model in tfidf.model.\n",
    "\n",
    "        Set tofull = True if the next thing is a Scikit-Learn estimator\n",
    "        otherwise keep False if the next thing is a Gensim model.\n",
    "        \"\"\"\n",
    "        self._lexicon_path = os.path.join(dirpath, \"corpus.dict\")\n",
    "        self._tfidf_path = os.path.join(dirpath, \"tfidf.model\")\n",
    "\n",
    "        self.lexicon = None\n",
    "        self.tfidf = None\n",
    "        self.tofull = tofull\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.exists(self._lexicon_path):\n",
    "            self.lexicon = Dictionary.load(self._lexicon_path)\n",
    "\n",
    "        if os.path.exists(self._tfidf_path):\n",
    "            self.tfidf = TfidfModel().load(self._tfidf_path)\n",
    "\n",
    "    def save(self):\n",
    "        self.lexicon.save(self._lexicon_path)\n",
    "        self.tfidf.save(self._tfidf_path)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        self.lexicon = Dictionary(documents)\n",
    "        self.tfidf = TfidfModel([\n",
    "            self.lexicon.doc2bow(doc)\n",
    "            for doc in documents],\n",
    "            id2word=self.lexicon)\n",
    "        self.save()\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        def generator():\n",
    "            for document in documents:\n",
    "                vec = self.tfidf[self.lexicon.doc2bow(document)]\n",
    "                if self.tofull:\n",
    "                    yield sparse2full(vec)\n",
    "                else:\n",
    "                    yield vec\n",
    "        return list(generator())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.sklearn_api import ldamodel\n",
    "\n",
    "\n",
    "class GensimTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('vect', GensimTfidfVectorizer()),\n",
    "            ('model', ldamodel.LdaTransformer(num_topics = self.n_topics))\n",
    "        ])\n",
    "\n",
    "    def fit(self, documents):\n",
    "        self.model.fit(documents)\n",
    "\n",
    "        return self.model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-5e608ed20f24>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'__main__'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPickledCorpusReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/sample'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mgensim_lda\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mGensimTopicModels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    corpus = PickledCorpusReader('../data/sample')\n",
    "\n",
    "    gensim_lda = GensimTopicModels()\n",
    "\n",
    "    docs = [\n",
    "        list(corpus.docs(fileids=fileid))[0]\n",
    "        for fileid in corpus.fileids()\n",
    "    ]\n",
    "\n",
    "    gensim_lda.fit(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-85db8d684733>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlda\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgensim_lda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnamed_steps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'model'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgensim_model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow_topics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'gensim_lda' is not defined"
     ]
    }
   ],
   "source": [
    "lda = gensim_lda.model.named_steps['model'].gensim_model\n",
    "print(lda.show_topics())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-99416a3a4a4c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtopics\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mlda\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgensim_lda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnamed_steps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'model'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgensim_model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m corpus = [\n",
      "\u001B[0;31mNameError\u001B[0m: name 'gensim_lda' is not defined"
     ]
    }
   ],
   "source": [
    "def get_topics(vectorized_corpus, model):\n",
    "    from operator import itemgetter\n",
    "    topics = [\n",
    "        max(model[doc], key=itemgetter(1))[0]\n",
    "        for doc in vectorized_corpus\n",
    "    ]\n",
    "\n",
    "    return topics\n",
    "\n",
    "lda = gensim_lda.model.named_steps['model'].gensim_model\n",
    "\n",
    "corpus = [\n",
    "    gensim_lda.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
    "    for doc in gensim_lda.model.named_steps['norm'].transform(docs)\n",
    "]\n",
    "\n",
    "topics = get_topics(corpus,lda)\n",
    "\n",
    "for topic, doc in zip(topics, docs):\n",
    "    print(\"Topic:{}\".format(topic))\n",
    "    print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-76ed88863499>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyLDAvis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgensim\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mlda\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgensim_lda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnamed_steps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'model'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgensim_model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m corpus = [\n",
      "\u001B[0;31mNameError\u001B[0m: name 'gensim_lda' is not defined"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "lda = gensim_lda.model.named_steps['model'].gensim_model\n",
    "\n",
    "corpus = [\n",
    "    gensim_lda.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
    "    for doc in gensim_lda.model.named_steps['norm'].transform(docs)\n",
    "]\n",
    "\n",
    "lexicon = gensim_lda.model.named_steps['vect'].lexicon\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(model,corpus,lexicon)\n",
    "pyLDAvis.display(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class SklearnTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA',\n",
    "        otherwise, defaults to Latent Dirichlet Allocation ('LDA').\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = LatentDirichletAllocation(n_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('tfidf', CountVectorizer(tokenizer=identity,\n",
    "                                      preprocessor=None, lowercase=False)),\n",
    "            ('model', self.estimator)\n",
    "        ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import lsimodel, ldamodel\n",
    "\n",
    "\n",
    "class GensimTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA'\n",
    "        otherwise defaults to Latent Dirichlet Allocation.\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = lsimodel.LsiTransformer(num_topics=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = ldamodel.LdaTransformer(num_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('vect', GensimTfidfVectorizer()),\n",
    "            ('model', self.estimator)\n",
    "        ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "class SklearnTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA',\n",
    "        To use Non-Negative Matrix Factorization, set estimator to 'NMF',\n",
    "        otherwise, defaults to Latent Dirichlet Allocation ('LDA').\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
    "        elif estimator == 'NMF':\n",
    "            self.estimator = NMF(n_components=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = LatentDirichletAllocation(n_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('tfidf', CountVectorizer(tokenizer=identity,\n",
    "                                      preprocessor=None, lowercase=False)),\n",
    "            ('model', self.estimator)\n",
    "        ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}